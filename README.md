# Sketch-based Face Recognition

## Description

In this blog, a research paper titled “Face recognition from multiple stylistic sketches: Scenarios, datasets, and evaluation” is explored. The aim behind the analysis is to present a new machine learning technique that can handle the sketch-based face recognition issue. The attached code at https://github.com/Saleh824/FaceRecognition is about-face recognition by CNN Model. Latterly, my aim is to advance the code for sketch-based face recognition.
Thy analysis of explored paper and the results of my simulation are in subsequent sections.


# Introduction

The challenge of recognizing faces from sketches has proven to be difficult. In the real world, security camera images and videos are of low quality and provide no clues or facts [1]. Because of this limitation, crime departments rely on forensic sketch artists [2] or sketching software [3] to sketch suspects' faces.  As a result of this gap, a complicated problem emerged: sketch-based face recognition.

The key challenge in face sketch recognition is the classification for a given face sketch mug shot or portrait of any suspect for identification using a stored database of actual face photographs of that suspect. The main feature of the sketch-based face recognition system is to classify the recently provided test sketch by training the face sketch recognition system with actual images from a stored database of identified persons' images into one of the groups. Face sketch recognition seems to be simple for human vision systems, but it can be difficult for machines with reduced knowledge and memory.

The challenge of linking facial sketch images with actual images has been solved using a number of sketch-based face recognition techniques. These methods of identification can be divided into two categories: discriminative and Generative methods. The generative approach entails converting a sketch picture to a real-life face image and then comparing it to a real-life image. The discriminative approach, on the other hand, is to fit the actual face picture by removing features from the sketch and stored databases.

This selected article provides a profound inquiry into the issue of facial recognition from several stylistic sketches. This is, to the author’s knowledge, the first thorough inquiry into the topic. Since there are two types of sketches available, hand-drawn sketches and software-generated composite sketches, this paper carefully defines three scenarios: 1. recognition of faces from multiple hand-drawn sketches made by various artists; 2. recognition of faces from both hand-drawn sketches and machine-generated composite sketches; 3. recognition of faces from multiple software-generated composite sketches generated by different algorithms. The related protocols and datasets are presented, as well as baseline output under the proposed protocols for measurement and comparison. This paper discusses the problems and alternative paths based on the experimental findings, which can be explored further in the future. The baselines presented in this paper are merely a starting point for the subject of facial recognition from several stylistic sketches.

## Dataset Used 

Three specific scenarios based on the face sketch styles and datasets available in the community: 
    •	Scenario-MHS: face recognition from multiple hand-drawn sketches, which are drawn by multiple forensic artists
    •	Scenario-MHCS: face recognition from both hand-drawn sketch and software-generated composite sketches
    •	Scenario-MCS: face recognition from multiple stylistic composite sketches, which are created by multiple software tools.
All of these scenarios are gathered from 10 different datasets shown in figure 1.

![image](https://user-images.githubusercontent.com/81626903/113000462-4a6aea80-9189-11eb-9ea0-44f75bf27d1e.png)


## Results

Results in figure 2 show that VGG-Net achieved the highest accuracy for all datasets. However, its accuracy for the last two (i.e. Software generated) datasets were not satisfactory. Therefore, one can work to improve the accuracy of these two datasets.

![image](https://user-images.githubusercontent.com/81626903/113000596-69697c80-9189-11eb-971e-58a2562f2568.png)


# Face Recognition (My code)

I used the ORL face database, which consists of 400 images with a resolution of 112 x 92 pixels. There are 40 people in total, each with ten pictures. The pictures were taken at various moments, with various lighting and facial expressions. In frontal view, the faces are in an upright posture with a slight left-right rotation.

The rest of the code and simulations are given below:


# Face Recognition using CNN

 # Step1:
 
At the first, you should input the required libraries:


```python
import keras
from keras.models import Sequential
from keras.layers import Conv2D, MaxPooling2D, Dense, Flatten, Dropout
from keras.optimizers import Adam
from keras.callbacks import TensorBoard

import numpy as np
import pandas as pd
import matplotlib.pyplot as plt
from sklearn.model_selection import train_test_split

from sklearn.metrics import confusion_matrix
from sklearn.metrics import classification_report
from sklearn.metrics import roc_curve, auc
from sklearn.metrics import accuracy_score
from keras.utils import np_utils
import itertools
```

# Step2:

* Load Dataset :

After loading the Dataset you have to normalize every image.

Note: an image is a Uint8 matrix of pixels and for calculation, you need to convert the format of the image to float or double


```python
#load dataset
data = np.load('ORL_faces.npz') 

# load the "Train Images"
x_train = data['trainX']
#normalize every image
x_train = np.array(x_train,dtype='float32')/255

x_test = data['testX']
x_test = np.array(x_test,dtype='float32')/255

# load the Label of Images
y_train= data['trainY']
y_test= data['testY']

# show the train and test Data format
print('x_train : {}'.format(x_train[:]))
print('Y-train shape: {}'.format(y_train))
print('x_test shape: {}'.format(x_test.shape))
```

    x_train : [[0.1882353  0.19215687 0.1764706  ... 0.18431373 0.18039216 0.18039216]
     [0.23529412 0.23529412 0.24313726 ... 0.1254902  0.13333334 0.13333334]
     [0.15294118 0.17254902 0.20784314 ... 0.11372549 0.10196079 0.11372549]
     ...
     [0.44705883 0.45882353 0.44705883 ... 0.38431373 0.3764706  0.38431373]
     [0.4117647  0.4117647  0.41960785 ... 0.21176471 0.18431373 0.16078432]
     [0.45490196 0.44705883 0.45882353 ... 0.37254903 0.39215687 0.39607844]]
    Y-train shape: [ 0  0  0  0  0  0  0  0  0  0  0  0  1  1  1  1  1  1  1  1  1  1  1  1
      2  2  2  2  2  2  2  2  2  2  2  2  3  3  3  3  3  3  3  3  3  3  3  3
      4  4  4  4  4  4  4  4  4  4  4  4  5  5  5  5  5  5  5  5  5  5  5  5
      6  6  6  6  6  6  6  6  6  6  6  6  7  7  7  7  7  7  7  7  7  7  7  7
      8  8  8  8  8  8  8  8  8  8  8  8  9  9  9  9  9  9  9  9  9  9  9  9
     10 10 10 10 10 10 10 10 10 10 10 10 11 11 11 11 11 11 11 11 11 11 11 11
     12 12 12 12 12 12 12 12 12 12 12 12 13 13 13 13 13 13 13 13 13 13 13 13
     14 14 14 14 14 14 14 14 14 14 14 14 15 15 15 15 15 15 15 15 15 15 15 15
     16 16 16 16 16 16 16 16 16 16 16 16 17 17 17 17 17 17 17 17 17 17 17 17
     18 18 18 18 18 18 18 18 18 18 18 18 19 19 19 19 19 19 19 19 19 19 19 19]
    x_test shape: (160, 10304)
    

# Step 3

Split DataSet : Validation data and Train 

Validation DataSet: this data set is used to minimize overfitting.If the accuracy over the training data set increases, but the accuracy over then validation data set stays the same or decreases, then you're overfitting your neural network and you should stop training.

 * Note: we usually use 30 percent of every dataset as the validation data but Here we only used 5 percent because the number of images in this dataset is very low. 




```python
x_train, x_valid, y_train, y_valid= train_test_split(
    x_train, y_train, test_size=.05, random_state=1234,)
```

# Step 4

for using the CNN, we need to change The size of images ( The size of images must be the same)


```python
im_rows=112
im_cols=92
batch_size=512
im_shape=(im_rows, im_cols, 1)

#change the size of images
x_train = x_train.reshape(x_train.shape[0], *im_shape)
x_test = x_test.reshape(x_test.shape[0], *im_shape)
x_valid = x_valid.reshape(x_valid.shape[0], *im_shape)

print('x_train shape: {}'.format(y_train.shape[0]))
print('x_test shape: {}'.format(y_test.shape))
```

    x_train shape: 228
    x_test shape: (160,)
    

# Step 5


Build CNN model:
 CNN have 3 main layer: 
 * 1-Convolotional layer 
 * 2- pooling layer  
 * 3- fully connected layer
 
 we could build a new architecture of CNN by changing the number and position of layers.
 


```python

#filters= the depth of output image or kernels

cnn_model= Sequential([
    Conv2D(filters=36, kernel_size=7, activation='relu', input_shape= im_shape),
    MaxPooling2D(pool_size=2),
    Conv2D(filters=54, kernel_size=5, activation='relu', input_shape= im_shape),
    MaxPooling2D(pool_size=2),
    Flatten(),
    Dense(2024, activation='relu'),
     Dropout(0.5),
    Dense(1024, activation='relu'),
    Dropout(0.5),
    Dense(512, activation='relu'),
    Dropout(0.5),
    #20 is the number of outputs
    Dense(20, activation='softmax')  
])

cnn_model.compile(
    loss='sparse_categorical_crossentropy',#'categorical_crossentropy',
    optimizer=Adam(lr=0.0001),
    metrics=['accuracy']
)
```

Show the model's parameters.


```python
cnn_model.summary()
```

    Model: "sequential"
    _________________________________________________________________
    Layer (type)                 Output Shape              Param #   
    =================================================================
    conv2d (Conv2D)              (None, 106, 86, 36)       1800      
    _________________________________________________________________
    max_pooling2d (MaxPooling2D) (None, 53, 43, 36)        0         
    _________________________________________________________________
    conv2d_1 (Conv2D)            (None, 49, 39, 54)        48654     
    _________________________________________________________________
    max_pooling2d_1 (MaxPooling2 (None, 24, 19, 54)        0         
    _________________________________________________________________
    flatten (Flatten)            (None, 24624)             0         
    _________________________________________________________________
    dense (Dense)                (None, 2024)              49841000  
    _________________________________________________________________
    dropout (Dropout)            (None, 2024)              0         
    _________________________________________________________________
    dense_1 (Dense)              (None, 1024)              2073600   
    _________________________________________________________________
    dropout_1 (Dropout)          (None, 1024)              0         
    _________________________________________________________________
    dense_2 (Dense)              (None, 512)               524800    
    _________________________________________________________________
    dropout_2 (Dropout)          (None, 512)               0         
    _________________________________________________________________
    dense_3 (Dense)              (None, 20)                10260     
    =================================================================
    Total params: 52,500,114
    Trainable params: 52,500,114
    Non-trainable params: 0
    _________________________________________________________________
    

# Step 6

Train the Model

* Note: You can change the number of epochs



```python
history=cnn_model.fit(
    np.array(x_train), np.array(y_train), batch_size=512,
    epochs=250, verbose=2,
    validation_data=(np.array(x_valid),np.array(y_valid)),
)
```

    Epoch 1/250
    1/1 - 14s - loss: 3.0112 - accuracy: 0.0526 - val_loss: 3.0060 - val_accuracy: 0.0000e+00
    Epoch 2/250
    1/1 - 7s - loss: 3.0183 - accuracy: 0.0395 - val_loss: 2.9973 - val_accuracy: 0.0833
    Epoch 3/250
    1/1 - 5s - loss: 2.9934 - accuracy: 0.0702 - val_loss: 2.9892 - val_accuracy: 0.1667
    Epoch 4/250
    1/1 - 4s - loss: 3.0081 - accuracy: 0.0132 - val_loss: 2.9885 - val_accuracy: 0.0833
    Epoch 5/250
    1/1 - 4s - loss: 3.0194 - accuracy: 0.0570 - val_loss: 2.9854 - val_accuracy: 0.0000e+00
    Epoch 6/250
    1/1 - 4s - loss: 3.0267 - accuracy: 0.0526 - val_loss: 2.9880 - val_accuracy: 0.0000e+00
    Epoch 7/250
    1/1 - 4s - loss: 3.0055 - accuracy: 0.0526 - val_loss: 2.9903 - val_accuracy: 0.0000e+00
    Epoch 8/250
    1/1 - 10s - loss: 2.9869 - accuracy: 0.0877 - val_loss: 2.9910 - val_accuracy: 0.0000e+00
    Epoch 9/250
    1/1 - 7s - loss: 2.9905 - accuracy: 0.0746 - val_loss: 2.9945 - val_accuracy: 0.0000e+00
    Epoch 10/250
    1/1 - 5s - loss: 2.9921 - accuracy: 0.0789 - val_loss: 2.9957 - val_accuracy: 0.0000e+00
    Epoch 11/250
    1/1 - 5s - loss: 2.9796 - accuracy: 0.0482 - val_loss: 2.9950 - val_accuracy: 0.0000e+00
    Epoch 12/250
    1/1 - 5s - loss: 2.9799 - accuracy: 0.0746 - val_loss: 2.9944 - val_accuracy: 0.0000e+00
    Epoch 13/250
    1/1 - 5s - loss: 2.9860 - accuracy: 0.0702 - val_loss: 2.9937 - val_accuracy: 0.0000e+00
    Epoch 14/250
    1/1 - 5s - loss: 2.9464 - accuracy: 0.1096 - val_loss: 2.9934 - val_accuracy: 0.0000e+00
    Epoch 15/250
    1/1 - 5s - loss: 2.9705 - accuracy: 0.0921 - val_loss: 2.9935 - val_accuracy: 0.0000e+00
    Epoch 16/250
    1/1 - 5s - loss: 2.9432 - accuracy: 0.0965 - val_loss: 2.9936 - val_accuracy: 0.0000e+00
    Epoch 17/250
    1/1 - 5s - loss: 2.9296 - accuracy: 0.1184 - val_loss: 2.9938 - val_accuracy: 0.0000e+00
    Epoch 18/250
    1/1 - 5s - loss: 2.9580 - accuracy: 0.0746 - val_loss: 2.9918 - val_accuracy: 0.0000e+00
    Epoch 19/250
    1/1 - 5s - loss: 2.9578 - accuracy: 0.0702 - val_loss: 2.9890 - val_accuracy: 0.0000e+00
    Epoch 20/250
    1/1 - 5s - loss: 2.9473 - accuracy: 0.1009 - val_loss: 2.9855 - val_accuracy: 0.0000e+00
    Epoch 21/250
    1/1 - 5s - loss: 2.9440 - accuracy: 0.1053 - val_loss: 2.9818 - val_accuracy: 0.0000e+00
    Epoch 22/250
    1/1 - 5s - loss: 2.9220 - accuracy: 0.1228 - val_loss: 2.9773 - val_accuracy: 0.0000e+00
    Epoch 23/250
    1/1 - 5s - loss: 2.9076 - accuracy: 0.1535 - val_loss: 2.9721 - val_accuracy: 0.0000e+00
    Epoch 24/250
    1/1 - 5s - loss: 2.9104 - accuracy: 0.1579 - val_loss: 2.9647 - val_accuracy: 0.0000e+00
    Epoch 25/250
    1/1 - 5s - loss: 2.8916 - accuracy: 0.1535 - val_loss: 2.9563 - val_accuracy: 0.0833
    Epoch 26/250
    1/1 - 5s - loss: 2.8806 - accuracy: 0.1623 - val_loss: 2.9478 - val_accuracy: 0.0833
    Epoch 27/250
    1/1 - 5s - loss: 2.8859 - accuracy: 0.1535 - val_loss: 2.9382 - val_accuracy: 0.0833
    Epoch 28/250
    1/1 - 5s - loss: 2.8699 - accuracy: 0.1711 - val_loss: 2.9284 - val_accuracy: 0.0833
    Epoch 29/250
    1/1 - 5s - loss: 2.8495 - accuracy: 0.1842 - val_loss: 2.9177 - val_accuracy: 0.0833
    Epoch 30/250
    1/1 - 5s - loss: 2.8342 - accuracy: 0.1623 - val_loss: 2.9063 - val_accuracy: 0.0833
    Epoch 31/250
    1/1 - 5s - loss: 2.8259 - accuracy: 0.1798 - val_loss: 2.8923 - val_accuracy: 0.0833
    Epoch 32/250
    1/1 - 5s - loss: 2.8189 - accuracy: 0.1842 - val_loss: 2.8727 - val_accuracy: 0.0833
    Epoch 33/250
    1/1 - 5s - loss: 2.7941 - accuracy: 0.1711 - val_loss: 2.8491 - val_accuracy: 0.1667
    Epoch 34/250
    1/1 - 5s - loss: 2.7648 - accuracy: 0.2325 - val_loss: 2.8207 - val_accuracy: 0.1667
    Epoch 35/250
    1/1 - 5s - loss: 2.7220 - accuracy: 0.2412 - val_loss: 2.7896 - val_accuracy: 0.2500
    Epoch 36/250
    1/1 - 5s - loss: 2.7329 - accuracy: 0.2237 - val_loss: 2.7542 - val_accuracy: 0.2500
    Epoch 37/250
    1/1 - 5s - loss: 2.6934 - accuracy: 0.2632 - val_loss: 2.7160 - val_accuracy: 0.2500
    Epoch 38/250
    1/1 - 5s - loss: 2.6755 - accuracy: 0.1974 - val_loss: 2.6751 - val_accuracy: 0.2500
    Epoch 39/250
    1/1 - 5s - loss: 2.6383 - accuracy: 0.2544 - val_loss: 2.6318 - val_accuracy: 0.3333
    Epoch 40/250
    1/1 - 5s - loss: 2.6146 - accuracy: 0.2456 - val_loss: 2.5874 - val_accuracy: 0.3333
    Epoch 41/250
    1/1 - 5s - loss: 2.5640 - accuracy: 0.2588 - val_loss: 2.5437 - val_accuracy: 0.3333
    Epoch 42/250
    1/1 - 5s - loss: 2.5087 - accuracy: 0.2675 - val_loss: 2.4924 - val_accuracy: 0.3333
    Epoch 43/250
    1/1 - 5s - loss: 2.4624 - accuracy: 0.3158 - val_loss: 2.4300 - val_accuracy: 0.3333
    Epoch 44/250
    1/1 - 5s - loss: 2.4398 - accuracy: 0.2982 - val_loss: 2.3759 - val_accuracy: 0.4167
    Epoch 45/250
    1/1 - 5s - loss: 2.4382 - accuracy: 0.3246 - val_loss: 2.3310 - val_accuracy: 0.4167
    Epoch 46/250
    1/1 - 5s - loss: 2.3586 - accuracy: 0.3333 - val_loss: 2.2896 - val_accuracy: 0.4167
    Epoch 47/250
    1/1 - 5s - loss: 2.3449 - accuracy: 0.3246 - val_loss: 2.2536 - val_accuracy: 0.4167
    Epoch 48/250
    1/1 - 5s - loss: 2.2291 - accuracy: 0.3684 - val_loss: 2.2033 - val_accuracy: 0.4167
    Epoch 49/250
    1/1 - 5s - loss: 2.2256 - accuracy: 0.3553 - val_loss: 2.1438 - val_accuracy: 0.4167
    Epoch 50/250
    1/1 - 5s - loss: 2.2399 - accuracy: 0.3509 - val_loss: 2.0863 - val_accuracy: 0.5000
    Epoch 51/250
    1/1 - 5s - loss: 2.0931 - accuracy: 0.3991 - val_loss: 2.0214 - val_accuracy: 0.5833
    Epoch 52/250
    1/1 - 5s - loss: 2.0321 - accuracy: 0.4035 - val_loss: 1.9565 - val_accuracy: 0.5833
    Epoch 53/250
    1/1 - 5s - loss: 1.9837 - accuracy: 0.4342 - val_loss: 1.8928 - val_accuracy: 0.5833
    Epoch 54/250
    1/1 - 5s - loss: 1.8969 - accuracy: 0.4342 - val_loss: 1.8310 - val_accuracy: 0.5000
    Epoch 55/250
    1/1 - 5s - loss: 1.9962 - accuracy: 0.3947 - val_loss: 1.7573 - val_accuracy: 0.5833
    Epoch 56/250
    1/1 - 5s - loss: 1.9010 - accuracy: 0.4254 - val_loss: 1.7025 - val_accuracy: 0.5833
    Epoch 57/250
    1/1 - 5s - loss: 1.8405 - accuracy: 0.4649 - val_loss: 1.6604 - val_accuracy: 0.5000
    Epoch 58/250
    1/1 - 5s - loss: 1.6863 - accuracy: 0.5044 - val_loss: 1.6258 - val_accuracy: 0.5000
    Epoch 59/250
    1/1 - 5s - loss: 1.7922 - accuracy: 0.4605 - val_loss: 1.5726 - val_accuracy: 0.5000
    Epoch 60/250
    1/1 - 5s - loss: 1.6362 - accuracy: 0.4912 - val_loss: 1.5038 - val_accuracy: 0.5833
    Epoch 61/250
    1/1 - 5s - loss: 1.5304 - accuracy: 0.5526 - val_loss: 1.4252 - val_accuracy: 0.5833
    Epoch 62/250
    1/1 - 5s - loss: 1.5175 - accuracy: 0.5658 - val_loss: 1.3689 - val_accuracy: 0.5833
    Epoch 63/250
    1/1 - 5s - loss: 1.5278 - accuracy: 0.5395 - val_loss: 1.3256 - val_accuracy: 0.5833
    Epoch 64/250
    1/1 - 5s - loss: 1.4491 - accuracy: 0.5526 - val_loss: 1.2596 - val_accuracy: 0.5833
    Epoch 65/250
    1/1 - 5s - loss: 1.4093 - accuracy: 0.5614 - val_loss: 1.1697 - val_accuracy: 0.6667
    Epoch 66/250
    1/1 - 5s - loss: 1.4013 - accuracy: 0.5482 - val_loss: 1.1046 - val_accuracy: 0.6667
    Epoch 67/250
    1/1 - 5s - loss: 1.2087 - accuracy: 0.6491 - val_loss: 1.0512 - val_accuracy: 0.7500
    Epoch 68/250
    1/1 - 5s - loss: 1.2771 - accuracy: 0.6009 - val_loss: 1.0228 - val_accuracy: 0.9167
    Epoch 69/250
    1/1 - 5s - loss: 1.2751 - accuracy: 0.6053 - val_loss: 0.9561 - val_accuracy: 0.9167
    Epoch 70/250
    1/1 - 5s - loss: 1.2545 - accuracy: 0.6096 - val_loss: 0.9044 - val_accuracy: 0.9167
    Epoch 71/250
    1/1 - 5s - loss: 1.1502 - accuracy: 0.6272 - val_loss: 0.8750 - val_accuracy: 0.9167
    Epoch 72/250
    1/1 - 5s - loss: 1.0339 - accuracy: 0.7237 - val_loss: 0.8532 - val_accuracy: 0.9167
    Epoch 73/250
    1/1 - 5s - loss: 0.9795 - accuracy: 0.7412 - val_loss: 0.8060 - val_accuracy: 0.9167
    Epoch 74/250
    1/1 - 5s - loss: 1.0554 - accuracy: 0.6974 - val_loss: 0.7735 - val_accuracy: 0.9167
    Epoch 75/250
    1/1 - 5s - loss: 0.8889 - accuracy: 0.7500 - val_loss: 0.7342 - val_accuracy: 0.9167
    Epoch 76/250
    1/1 - 5s - loss: 0.9127 - accuracy: 0.7281 - val_loss: 0.6945 - val_accuracy: 0.9167
    Epoch 77/250
    1/1 - 5s - loss: 0.9251 - accuracy: 0.7281 - val_loss: 0.6505 - val_accuracy: 0.9167
    Epoch 78/250
    1/1 - 5s - loss: 0.8509 - accuracy: 0.7588 - val_loss: 0.5835 - val_accuracy: 0.9167
    Epoch 79/250
    1/1 - 5s - loss: 0.8166 - accuracy: 0.7544 - val_loss: 0.5386 - val_accuracy: 0.9167
    Epoch 80/250
    1/1 - 5s - loss: 0.7150 - accuracy: 0.7763 - val_loss: 0.4935 - val_accuracy: 0.9167
    Epoch 81/250
    1/1 - 5s - loss: 0.7814 - accuracy: 0.7500 - val_loss: 0.4649 - val_accuracy: 0.9167
    Epoch 82/250
    1/1 - 5s - loss: 0.8362 - accuracy: 0.7368 - val_loss: 0.4658 - val_accuracy: 0.9167
    Epoch 83/250
    1/1 - 5s - loss: 0.6970 - accuracy: 0.8158 - val_loss: 0.4318 - val_accuracy: 0.9167
    Epoch 84/250
    1/1 - 5s - loss: 0.6909 - accuracy: 0.7982 - val_loss: 0.4160 - val_accuracy: 0.9167
    Epoch 85/250
    1/1 - 5s - loss: 0.6614 - accuracy: 0.7939 - val_loss: 0.4080 - val_accuracy: 0.9167
    Epoch 86/250
    1/1 - 5s - loss: 0.6129 - accuracy: 0.8246 - val_loss: 0.4144 - val_accuracy: 0.9167
    Epoch 87/250
    1/1 - 5s - loss: 0.5784 - accuracy: 0.8377 - val_loss: 0.4112 - val_accuracy: 0.9167
    Epoch 88/250
    1/1 - 5s - loss: 0.5311 - accuracy: 0.8553 - val_loss: 0.3790 - val_accuracy: 0.9167
    Epoch 89/250
    1/1 - 5s - loss: 0.5593 - accuracy: 0.8246 - val_loss: 0.3458 - val_accuracy: 0.9167
    Epoch 90/250
    1/1 - 5s - loss: 0.5535 - accuracy: 0.8465 - val_loss: 0.3300 - val_accuracy: 0.9167
    Epoch 91/250
    1/1 - 5s - loss: 0.5052 - accuracy: 0.8509 - val_loss: 0.3079 - val_accuracy: 0.9167
    Epoch 92/250
    1/1 - 5s - loss: 0.4116 - accuracy: 0.9035 - val_loss: 0.3145 - val_accuracy: 0.9167
    Epoch 93/250
    1/1 - 5s - loss: 0.4527 - accuracy: 0.8465 - val_loss: 0.2855 - val_accuracy: 0.9167
    Epoch 94/250
    1/1 - 5s - loss: 0.4341 - accuracy: 0.8904 - val_loss: 0.2387 - val_accuracy: 0.9167
    Epoch 95/250
    1/1 - 5s - loss: 0.3937 - accuracy: 0.9035 - val_loss: 0.2213 - val_accuracy: 0.9167
    Epoch 96/250
    1/1 - 5s - loss: 0.4321 - accuracy: 0.8728 - val_loss: 0.2510 - val_accuracy: 0.9167
    Epoch 97/250
    1/1 - 5s - loss: 0.3556 - accuracy: 0.8947 - val_loss: 0.2554 - val_accuracy: 0.9167
    Epoch 98/250
    1/1 - 5s - loss: 0.3717 - accuracy: 0.8904 - val_loss: 0.2071 - val_accuracy: 0.9167
    Epoch 99/250
    1/1 - 5s - loss: 0.3701 - accuracy: 0.9079 - val_loss: 0.1684 - val_accuracy: 0.9167
    Epoch 100/250
    1/1 - 5s - loss: 0.3442 - accuracy: 0.8816 - val_loss: 0.1593 - val_accuracy: 0.9167
    Epoch 101/250
    1/1 - 5s - loss: 0.3119 - accuracy: 0.9079 - val_loss: 0.1722 - val_accuracy: 0.9167
    Epoch 102/250
    1/1 - 5s - loss: 0.2485 - accuracy: 0.9254 - val_loss: 0.1898 - val_accuracy: 0.9167
    Epoch 103/250
    1/1 - 5s - loss: 0.2832 - accuracy: 0.9342 - val_loss: 0.1874 - val_accuracy: 0.9167
    Epoch 104/250
    1/1 - 5s - loss: 0.3368 - accuracy: 0.9254 - val_loss: 0.1620 - val_accuracy: 0.9167
    Epoch 105/250
    1/1 - 5s - loss: 0.2732 - accuracy: 0.8991 - val_loss: 0.1394 - val_accuracy: 0.9167
    Epoch 106/250
    1/1 - 5s - loss: 0.2656 - accuracy: 0.9254 - val_loss: 0.1460 - val_accuracy: 0.9167
    Epoch 107/250
    1/1 - 5s - loss: 0.2727 - accuracy: 0.9167 - val_loss: 0.1595 - val_accuracy: 0.9167
    Epoch 108/250
    1/1 - 5s - loss: 0.2913 - accuracy: 0.9079 - val_loss: 0.1743 - val_accuracy: 0.9167
    Epoch 109/250
    1/1 - 5s - loss: 0.2088 - accuracy: 0.9474 - val_loss: 0.1560 - val_accuracy: 0.9167
    Epoch 110/250
    1/1 - 5s - loss: 0.2410 - accuracy: 0.9298 - val_loss: 0.1391 - val_accuracy: 0.9167
    Epoch 111/250
    1/1 - 5s - loss: 0.2277 - accuracy: 0.9386 - val_loss: 0.1379 - val_accuracy: 0.9167
    Epoch 112/250
    1/1 - 5s - loss: 0.2118 - accuracy: 0.9649 - val_loss: 0.1494 - val_accuracy: 0.9167
    Epoch 113/250
    1/1 - 5s - loss: 0.1999 - accuracy: 0.9561 - val_loss: 0.1394 - val_accuracy: 0.9167
    Epoch 114/250
    1/1 - 5s - loss: 0.2115 - accuracy: 0.9430 - val_loss: 0.1131 - val_accuracy: 1.0000
    Epoch 115/250
    1/1 - 5s - loss: 0.1631 - accuracy: 0.9737 - val_loss: 0.1018 - val_accuracy: 1.0000
    Epoch 116/250
    1/1 - 5s - loss: 0.2096 - accuracy: 0.9518 - val_loss: 0.0962 - val_accuracy: 1.0000
    Epoch 117/250
    1/1 - 5s - loss: 0.2114 - accuracy: 0.9298 - val_loss: 0.0917 - val_accuracy: 1.0000
    Epoch 118/250
    1/1 - 5s - loss: 0.1740 - accuracy: 0.9430 - val_loss: 0.0866 - val_accuracy: 1.0000
    Epoch 119/250
    1/1 - 5s - loss: 0.1478 - accuracy: 0.9605 - val_loss: 0.0902 - val_accuracy: 1.0000
    Epoch 120/250
    1/1 - 5s - loss: 0.1821 - accuracy: 0.9649 - val_loss: 0.0767 - val_accuracy: 1.0000
    Epoch 121/250
    1/1 - 5s - loss: 0.1333 - accuracy: 0.9737 - val_loss: 0.0594 - val_accuracy: 1.0000
    Epoch 122/250
    1/1 - 5s - loss: 0.1546 - accuracy: 0.9649 - val_loss: 0.0613 - val_accuracy: 1.0000
    Epoch 123/250
    1/1 - 5s - loss: 0.1557 - accuracy: 0.9649 - val_loss: 0.0778 - val_accuracy: 1.0000
    Epoch 124/250
    1/1 - 5s - loss: 0.1457 - accuracy: 0.9693 - val_loss: 0.1109 - val_accuracy: 0.9167
    Epoch 125/250
    1/1 - 5s - loss: 0.1429 - accuracy: 0.9561 - val_loss: 0.1345 - val_accuracy: 0.9167
    Epoch 126/250
    1/1 - 5s - loss: 0.1668 - accuracy: 0.9561 - val_loss: 0.0757 - val_accuracy: 1.0000
    Epoch 127/250
    1/1 - 5s - loss: 0.1257 - accuracy: 0.9693 - val_loss: 0.0360 - val_accuracy: 1.0000
    Epoch 128/250
    1/1 - 5s - loss: 0.1494 - accuracy: 0.9561 - val_loss: 0.0287 - val_accuracy: 1.0000
    Epoch 129/250
    1/1 - 5s - loss: 0.1091 - accuracy: 0.9737 - val_loss: 0.0310 - val_accuracy: 1.0000
    Epoch 130/250
    1/1 - 5s - loss: 0.1495 - accuracy: 0.9649 - val_loss: 0.0362 - val_accuracy: 1.0000
    Epoch 131/250
    1/1 - 5s - loss: 0.1090 - accuracy: 0.9693 - val_loss: 0.0514 - val_accuracy: 1.0000
    Epoch 132/250
    1/1 - 5s - loss: 0.1040 - accuracy: 0.9737 - val_loss: 0.0769 - val_accuracy: 1.0000
    Epoch 133/250
    1/1 - 5s - loss: 0.1113 - accuracy: 0.9649 - val_loss: 0.0893 - val_accuracy: 1.0000
    Epoch 134/250
    1/1 - 5s - loss: 0.1091 - accuracy: 0.9781 - val_loss: 0.0567 - val_accuracy: 1.0000
    Epoch 135/250
    1/1 - 5s - loss: 0.0965 - accuracy: 0.9825 - val_loss: 0.0275 - val_accuracy: 1.0000
    Epoch 136/250
    1/1 - 5s - loss: 0.0861 - accuracy: 0.9868 - val_loss: 0.0175 - val_accuracy: 1.0000
    Epoch 137/250
    1/1 - 5s - loss: 0.0886 - accuracy: 0.9781 - val_loss: 0.0146 - val_accuracy: 1.0000
    Epoch 138/250
    1/1 - 5s - loss: 0.1098 - accuracy: 0.9737 - val_loss: 0.0142 - val_accuracy: 1.0000
    Epoch 139/250
    1/1 - 5s - loss: 0.0997 - accuracy: 0.9781 - val_loss: 0.0237 - val_accuracy: 1.0000
    Epoch 140/250
    1/1 - 5s - loss: 0.0607 - accuracy: 0.9868 - val_loss: 0.0548 - val_accuracy: 1.0000
    Epoch 141/250
    1/1 - 5s - loss: 0.0856 - accuracy: 0.9825 - val_loss: 0.0934 - val_accuracy: 1.0000
    Epoch 142/250
    1/1 - 5s - loss: 0.1789 - accuracy: 0.9430 - val_loss: 0.0534 - val_accuracy: 1.0000
    Epoch 143/250
    1/1 - 5s - loss: 0.0816 - accuracy: 0.9693 - val_loss: 0.0204 - val_accuracy: 1.0000
    Epoch 144/250
    1/1 - 5s - loss: 0.0695 - accuracy: 0.9868 - val_loss: 0.0093 - val_accuracy: 1.0000
    Epoch 145/250
    1/1 - 5s - loss: 0.1118 - accuracy: 0.9649 - val_loss: 0.0082 - val_accuracy: 1.0000
    Epoch 146/250
    1/1 - 5s - loss: 0.0771 - accuracy: 0.9825 - val_loss: 0.0089 - val_accuracy: 1.0000
    Epoch 147/250
    1/1 - 5s - loss: 0.0660 - accuracy: 0.9868 - val_loss: 0.0121 - val_accuracy: 1.0000
    Epoch 148/250
    1/1 - 5s - loss: 0.0830 - accuracy: 0.9781 - val_loss: 0.0195 - val_accuracy: 1.0000
    Epoch 149/250
    1/1 - 5s - loss: 0.0564 - accuracy: 1.0000 - val_loss: 0.0257 - val_accuracy: 1.0000
    Epoch 150/250
    1/1 - 5s - loss: 0.0582 - accuracy: 0.9912 - val_loss: 0.0238 - val_accuracy: 1.0000
    Epoch 151/250
    1/1 - 5s - loss: 0.0697 - accuracy: 0.9737 - val_loss: 0.0179 - val_accuracy: 1.0000
    Epoch 152/250
    1/1 - 5s - loss: 0.0650 - accuracy: 0.9825 - val_loss: 0.0126 - val_accuracy: 1.0000
    Epoch 153/250
    1/1 - 5s - loss: 0.0685 - accuracy: 0.9825 - val_loss: 0.0112 - val_accuracy: 1.0000
    Epoch 154/250
    1/1 - 5s - loss: 0.0795 - accuracy: 0.9781 - val_loss: 0.0121 - val_accuracy: 1.0000
    Epoch 155/250
    1/1 - 5s - loss: 0.0810 - accuracy: 0.9737 - val_loss: 0.0133 - val_accuracy: 1.0000
    Epoch 156/250
    1/1 - 5s - loss: 0.0387 - accuracy: 0.9956 - val_loss: 0.0143 - val_accuracy: 1.0000
    Epoch 157/250
    1/1 - 5s - loss: 0.0603 - accuracy: 0.9868 - val_loss: 0.0162 - val_accuracy: 1.0000
    Epoch 158/250
    1/1 - 5s - loss: 0.0521 - accuracy: 0.9912 - val_loss: 0.0175 - val_accuracy: 1.0000
    Epoch 159/250
    1/1 - 5s - loss: 0.0919 - accuracy: 0.9693 - val_loss: 0.0152 - val_accuracy: 1.0000
    Epoch 160/250
    1/1 - 5s - loss: 0.0676 - accuracy: 0.9781 - val_loss: 0.0118 - val_accuracy: 1.0000
    Epoch 161/250
    1/1 - 5s - loss: 0.0563 - accuracy: 0.9956 - val_loss: 0.0095 - val_accuracy: 1.0000
    Epoch 162/250
    1/1 - 5s - loss: 0.0455 - accuracy: 1.0000 - val_loss: 0.0102 - val_accuracy: 1.0000
    Epoch 163/250
    1/1 - 5s - loss: 0.0501 - accuracy: 0.9912 - val_loss: 0.0150 - val_accuracy: 1.0000
    Epoch 164/250
    1/1 - 5s - loss: 0.0508 - accuracy: 0.9868 - val_loss: 0.0210 - val_accuracy: 1.0000
    Epoch 165/250
    1/1 - 5s - loss: 0.0601 - accuracy: 0.9912 - val_loss: 0.0237 - val_accuracy: 1.0000
    Epoch 166/250
    1/1 - 5s - loss: 0.0433 - accuracy: 0.9825 - val_loss: 0.0235 - val_accuracy: 1.0000
    Epoch 167/250
    1/1 - 5s - loss: 0.0515 - accuracy: 0.9868 - val_loss: 0.0180 - val_accuracy: 1.0000
    Epoch 168/250
    1/1 - 5s - loss: 0.0411 - accuracy: 0.9912 - val_loss: 0.0150 - val_accuracy: 1.0000
    Epoch 169/250
    1/1 - 5s - loss: 0.0467 - accuracy: 0.9912 - val_loss: 0.0116 - val_accuracy: 1.0000
    Epoch 170/250
    1/1 - 5s - loss: 0.0394 - accuracy: 1.0000 - val_loss: 0.0093 - val_accuracy: 1.0000
    Epoch 171/250
    1/1 - 5s - loss: 0.0451 - accuracy: 0.9912 - val_loss: 0.0083 - val_accuracy: 1.0000
    Epoch 172/250
    1/1 - 5s - loss: 0.0618 - accuracy: 0.9781 - val_loss: 0.0096 - val_accuracy: 1.0000
    Epoch 173/250
    1/1 - 5s - loss: 0.0485 - accuracy: 0.9868 - val_loss: 0.0105 - val_accuracy: 1.0000
    Epoch 174/250
    1/1 - 5s - loss: 0.0406 - accuracy: 0.9912 - val_loss: 0.0114 - val_accuracy: 1.0000
    Epoch 175/250
    1/1 - 5s - loss: 0.0453 - accuracy: 0.9868 - val_loss: 0.0115 - val_accuracy: 1.0000
    Epoch 176/250
    1/1 - 5s - loss: 0.0490 - accuracy: 0.9912 - val_loss: 0.0127 - val_accuracy: 1.0000
    Epoch 177/250
    1/1 - 5s - loss: 0.0275 - accuracy: 0.9956 - val_loss: 0.0128 - val_accuracy: 1.0000
    Epoch 178/250
    1/1 - 5s - loss: 0.0252 - accuracy: 1.0000 - val_loss: 0.0126 - val_accuracy: 1.0000
    Epoch 179/250
    1/1 - 5s - loss: 0.0516 - accuracy: 0.9781 - val_loss: 0.0087 - val_accuracy: 1.0000
    Epoch 180/250
    1/1 - 5s - loss: 0.0294 - accuracy: 0.9956 - val_loss: 0.0061 - val_accuracy: 1.0000
    Epoch 181/250
    1/1 - 5s - loss: 0.0359 - accuracy: 0.9868 - val_loss: 0.0053 - val_accuracy: 1.0000
    Epoch 182/250
    1/1 - 5s - loss: 0.0534 - accuracy: 0.9868 - val_loss: 0.0046 - val_accuracy: 1.0000
    Epoch 183/250
    1/1 - 5s - loss: 0.0346 - accuracy: 0.9956 - val_loss: 0.0046 - val_accuracy: 1.0000
    Epoch 184/250
    1/1 - 5s - loss: 0.0391 - accuracy: 0.9956 - val_loss: 0.0048 - val_accuracy: 1.0000
    Epoch 185/250
    1/1 - 5s - loss: 0.0262 - accuracy: 1.0000 - val_loss: 0.0059 - val_accuracy: 1.0000
    Epoch 186/250
    1/1 - 5s - loss: 0.0203 - accuracy: 1.0000 - val_loss: 0.0077 - val_accuracy: 1.0000
    Epoch 187/250
    1/1 - 5s - loss: 0.0280 - accuracy: 1.0000 - val_loss: 0.0105 - val_accuracy: 1.0000
    Epoch 188/250
    1/1 - 5s - loss: 0.0272 - accuracy: 1.0000 - val_loss: 0.0146 - val_accuracy: 1.0000
    Epoch 189/250
    1/1 - 5s - loss: 0.0387 - accuracy: 0.9956 - val_loss: 0.0177 - val_accuracy: 1.0000
    Epoch 190/250
    1/1 - 5s - loss: 0.0325 - accuracy: 0.9912 - val_loss: 0.0189 - val_accuracy: 1.0000
    Epoch 191/250
    1/1 - 5s - loss: 0.0382 - accuracy: 0.9912 - val_loss: 0.0164 - val_accuracy: 1.0000
    Epoch 192/250
    1/1 - 5s - loss: 0.0288 - accuracy: 0.9956 - val_loss: 0.0116 - val_accuracy: 1.0000
    Epoch 193/250
    1/1 - 5s - loss: 0.0274 - accuracy: 0.9912 - val_loss: 0.0080 - val_accuracy: 1.0000
    Epoch 194/250
    1/1 - 5s - loss: 0.0210 - accuracy: 0.9912 - val_loss: 0.0045 - val_accuracy: 1.0000
    Epoch 195/250
    1/1 - 5s - loss: 0.0300 - accuracy: 0.9956 - val_loss: 0.0025 - val_accuracy: 1.0000
    Epoch 196/250
    1/1 - 5s - loss: 0.0300 - accuracy: 0.9912 - val_loss: 0.0018 - val_accuracy: 1.0000
    Epoch 197/250
    1/1 - 5s - loss: 0.0212 - accuracy: 1.0000 - val_loss: 0.0016 - val_accuracy: 1.0000
    Epoch 198/250
    1/1 - 5s - loss: 0.0480 - accuracy: 0.9825 - val_loss: 0.0022 - val_accuracy: 1.0000
    Epoch 199/250
    1/1 - 5s - loss: 0.0192 - accuracy: 0.9956 - val_loss: 0.0034 - val_accuracy: 1.0000
    Epoch 200/250
    1/1 - 5s - loss: 0.0449 - accuracy: 0.9912 - val_loss: 0.0040 - val_accuracy: 1.0000
    Epoch 201/250
    1/1 - 5s - loss: 0.0215 - accuracy: 0.9956 - val_loss: 0.0045 - val_accuracy: 1.0000
    Epoch 202/250
    1/1 - 5s - loss: 0.0295 - accuracy: 0.9912 - val_loss: 0.0036 - val_accuracy: 1.0000
    Epoch 203/250
    1/1 - 5s - loss: 0.0241 - accuracy: 0.9956 - val_loss: 0.0026 - val_accuracy: 1.0000
    Epoch 204/250
    1/1 - 5s - loss: 0.0393 - accuracy: 0.9825 - val_loss: 0.0018 - val_accuracy: 1.0000
    Epoch 205/250
    1/1 - 5s - loss: 0.0148 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000
    Epoch 206/250
    1/1 - 5s - loss: 0.0519 - accuracy: 0.9912 - val_loss: 0.0010 - val_accuracy: 1.0000
    Epoch 207/250
    1/1 - 5s - loss: 0.0187 - accuracy: 1.0000 - val_loss: 9.5469e-04 - val_accuracy: 1.0000
    Epoch 208/250
    1/1 - 5s - loss: 0.0218 - accuracy: 0.9956 - val_loss: 0.0011 - val_accuracy: 1.0000
    Epoch 209/250
    1/1 - 5s - loss: 0.0221 - accuracy: 0.9956 - val_loss: 0.0015 - val_accuracy: 1.0000
    Epoch 210/250
    1/1 - 5s - loss: 0.0207 - accuracy: 0.9912 - val_loss: 0.0021 - val_accuracy: 1.0000
    Epoch 211/250
    1/1 - 5s - loss: 0.0248 - accuracy: 0.9956 - val_loss: 0.0028 - val_accuracy: 1.0000
    Epoch 212/250
    1/1 - 5s - loss: 0.0351 - accuracy: 0.9912 - val_loss: 0.0037 - val_accuracy: 1.0000
    Epoch 213/250
    1/1 - 5s - loss: 0.0354 - accuracy: 0.9912 - val_loss: 0.0050 - val_accuracy: 1.0000
    Epoch 214/250
    1/1 - 5s - loss: 0.0250 - accuracy: 0.9912 - val_loss: 0.0047 - val_accuracy: 1.0000
    Epoch 215/250
    1/1 - 5s - loss: 0.0183 - accuracy: 1.0000 - val_loss: 0.0042 - val_accuracy: 1.0000
    Epoch 216/250
    1/1 - 5s - loss: 0.0209 - accuracy: 1.0000 - val_loss: 0.0035 - val_accuracy: 1.0000
    Epoch 217/250
    1/1 - 5s - loss: 0.0234 - accuracy: 0.9912 - val_loss: 0.0025 - val_accuracy: 1.0000
    Epoch 218/250
    1/1 - 5s - loss: 0.0143 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000
    Epoch 219/250
    1/1 - 5s - loss: 0.0175 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000
    Epoch 220/250
    1/1 - 5s - loss: 0.0244 - accuracy: 0.9956 - val_loss: 0.0011 - val_accuracy: 1.0000
    Epoch 221/250
    1/1 - 5s - loss: 0.0147 - accuracy: 0.9956 - val_loss: 9.9161e-04 - val_accuracy: 1.0000
    Epoch 222/250
    1/1 - 5s - loss: 0.0226 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000
    Epoch 223/250
    1/1 - 5s - loss: 0.0146 - accuracy: 1.0000 - val_loss: 0.0017 - val_accuracy: 1.0000
    Epoch 224/250
    1/1 - 5s - loss: 0.0116 - accuracy: 1.0000 - val_loss: 0.0026 - val_accuracy: 1.0000
    Epoch 225/250
    1/1 - 5s - loss: 0.0245 - accuracy: 0.9956 - val_loss: 0.0041 - val_accuracy: 1.0000
    Epoch 226/250
    1/1 - 5s - loss: 0.0210 - accuracy: 0.9912 - val_loss: 0.0048 - val_accuracy: 1.0000
    Epoch 227/250
    1/1 - 5s - loss: 0.0329 - accuracy: 0.9868 - val_loss: 0.0043 - val_accuracy: 1.0000
    Epoch 228/250
    1/1 - 5s - loss: 0.0238 - accuracy: 0.9912 - val_loss: 0.0034 - val_accuracy: 1.0000
    Epoch 229/250
    1/1 - 5s - loss: 0.0220 - accuracy: 0.9956 - val_loss: 0.0027 - val_accuracy: 1.0000
    Epoch 230/250
    1/1 - 5s - loss: 0.0185 - accuracy: 1.0000 - val_loss: 0.0023 - val_accuracy: 1.0000
    Epoch 231/250
    1/1 - 5s - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.0020 - val_accuracy: 1.0000
    Epoch 232/250
    1/1 - 5s - loss: 0.0159 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000
    Epoch 233/250
    1/1 - 5s - loss: 0.0185 - accuracy: 0.9956 - val_loss: 0.0024 - val_accuracy: 1.0000
    Epoch 234/250
    1/1 - 5s - loss: 0.0223 - accuracy: 1.0000 - val_loss: 0.0032 - val_accuracy: 1.0000
    Epoch 235/250
    1/1 - 5s - loss: 0.0105 - accuracy: 1.0000 - val_loss: 0.0037 - val_accuracy: 1.0000
    Epoch 236/250
    1/1 - 5s - loss: 0.0236 - accuracy: 0.9956 - val_loss: 0.0031 - val_accuracy: 1.0000
    Epoch 237/250
    1/1 - 5s - loss: 0.0185 - accuracy: 0.9956 - val_loss: 0.0025 - val_accuracy: 1.0000
    Epoch 238/250
    1/1 - 5s - loss: 0.0145 - accuracy: 0.9956 - val_loss: 0.0021 - val_accuracy: 1.0000
    Epoch 239/250
    1/1 - 5s - loss: 0.0121 - accuracy: 1.0000 - val_loss: 0.0018 - val_accuracy: 1.0000
    Epoch 240/250
    1/1 - 5s - loss: 0.0177 - accuracy: 0.9956 - val_loss: 0.0016 - val_accuracy: 1.0000
    Epoch 241/250
    1/1 - 5s - loss: 0.0303 - accuracy: 0.9868 - val_loss: 0.0014 - val_accuracy: 1.0000
    Epoch 242/250
    1/1 - 5s - loss: 0.0130 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000
    Epoch 243/250
    1/1 - 5s - loss: 0.0204 - accuracy: 0.9956 - val_loss: 0.0012 - val_accuracy: 1.0000
    Epoch 244/250
    1/1 - 5s - loss: 0.0125 - accuracy: 1.0000 - val_loss: 0.0013 - val_accuracy: 1.0000
    Epoch 245/250
    1/1 - 5s - loss: 0.0184 - accuracy: 0.9956 - val_loss: 0.0014 - val_accuracy: 1.0000
    Epoch 246/250
    1/1 - 5s - loss: 0.0259 - accuracy: 0.9868 - val_loss: 0.0015 - val_accuracy: 1.0000
    Epoch 247/250
    1/1 - 5s - loss: 0.0115 - accuracy: 1.0000 - val_loss: 0.0014 - val_accuracy: 1.0000
    Epoch 248/250
    1/1 - 5s - loss: 0.0138 - accuracy: 1.0000 - val_loss: 0.0012 - val_accuracy: 1.0000
    Epoch 249/250
    1/1 - 5s - loss: 0.0161 - accuracy: 0.9956 - val_loss: 8.0768e-04 - val_accuracy: 1.0000
    Epoch 250/250
    1/1 - 5s - loss: 0.0114 - accuracy: 1.0000 - val_loss: 6.6399e-04 - val_accuracy: 1.0000
    

Evaluate the test data



```python
scor = cnn_model.evaluate( np.array(x_test),  np.array(y_test), verbose=0)

print('test los {:.4f}'.format(scor[0]))
print('test acc {:.4f}'.format(scor[1]))
```

    test los 0.3208
    test acc 0.9375
    

# Step 7 

plot the result


```python
# list all data in history
print(history.history.keys())
# summarize history for accuracy
plt.plot(history.history['accuracy'])
plt.plot(history.history['val_accuracy'])
plt.title('model accuracy')
plt.ylabel('accuracy')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
# summarize history for loss
plt.plot(history.history['loss'])
plt.plot(history.history['val_loss'])
plt.title('model loss')
plt.ylabel('loss')
plt.xlabel('epoch')
plt.legend(['train', 'test'], loc='upper left')
plt.show()
```

    dict_keys(['loss', 'accuracy', 'val_loss', 'val_accuracy'])
    


    
![image](https://user-images.githubusercontent.com/81626903/113000919-b8171680-9189-11eb-8b02-a8a1d49282e5.png)

    

![image](https://user-images.githubusercontent.com/81626903/113000987-c82ef600-9189-11eb-94fe-d7deaf0d9c0d.png)


    

    


# step 8

Plot Confusion Matrix


```python
predicted =np.array( cnn_model.predict(x_test))
#print(predicted)
#print(y_test)
ynew = cnn_model.predict_classes(x_test)


Acc=accuracy_score(y_test, ynew)
print("accuracy : ")
print(Acc)
#/tn, fp, fn, tp = confusion_matrix(np.array(y_test), ynew).ravel()
cnf_matrix=confusion_matrix(np.array(y_test), ynew)

y_test1 = np_utils.to_categorical(y_test, 20)



def plot_confusion_matrix(cm, classes,
                          normalize=False,
                          title='Confusion matrix',
                          cmap=plt.cm.Blues):
    """
    This function prints and plots the confusion matrix.
    Normalization can be applied by setting `normalize=True`.
    """
    if normalize:
        cm = cm.astype('float') / cm.sum(axis=1)[:, np.newaxis]
        #print("Normalized confusion matrix")
    else:
        print('Confusion matrix, without normalization')

    #print(cm)
    plt.imshow(cm, interpolation='nearest', cmap=cmap)
    plt.title(title)
    plt.colorbar()
    tick_marks = np.arange(len(classes))
    plt.xticks(tick_marks, classes, rotation=45)
    plt.yticks(tick_marks, classes)

    fmt = '.2f' if normalize else 'd'
    thresh = cm.max() / 2.
    for i, j in itertools.product(range(cm.shape[0]), range(cm.shape[1])):
        plt.text(j, i, format(cm[i, j], fmt),
                 horizontalalignment="center",
                 color="white" if cm[i, j] > thresh else "black")

    plt.tight_layout()
    plt.ylabel('True label')
    plt.xlabel('Predicted label')
    plt.show()


print('Confusion matrix, without normalization')
print(cnf_matrix)

plt.figure()
plot_confusion_matrix(cnf_matrix[1:10,1:10], classes=[0,1,2,3,4,5,6,7,8,9],
                      title='Confusion matrix, without normalization')

plt.figure()
plot_confusion_matrix(cnf_matrix[11:20,11:20], classes=[10,11,12,13,14,15,16,17,18,19],
                      title='Confusion matrix, without normalization')

print("Confusion matrix:\n%s" % confusion_matrix(np.array(y_test), ynew))
print(classification_report(np.array(y_test), ynew))
```

    c:\users\temporary\appdata\local\programs\python\python38\lib\site-packages\tensorflow\python\keras\engine\sequential.py:450: UserWarning: `model.predict_classes()` is deprecated and will be removed after 2021-01-01. Please use instead:* `np.argmax(model.predict(x), axis=-1)`,   if your model does multi-class classification   (e.g. if it uses a `softmax` last-layer activation).* `(model.predict(x) > 0.5).astype("int32")`,   if your model does binary classification   (e.g. if it uses a `sigmoid` last-layer activation).
      warnings.warn('`model.predict_classes()` is deprecated and '
    

    accuracy : 
    0.9375
    Confusion matrix, without normalization
    [[8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
     [0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
     [0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
     [0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
     [0 0 0 0 6 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]
     [0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
     [0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0]
     [0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0]
     [0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0]
     [0 0 0 0 0 0 0 2 0 6 0 0 0 0 0 0 0 0 0 0]
     [0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0]
     [0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0]
     [0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0]
     [0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0]
     [0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0]
     [1 0 0 0 0 0 0 0 0 0 0 0 3 0 0 4 0 0 0 0]
     [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0]
     [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0]
     [0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 6 0]
     [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8]]
    Confusion matrix, without normalization
    


    
![image](https://user-images.githubusercontent.com/81626903/113001173-fdd3df00-9189-11eb-8bcb-88ac7647cdfb.png)

    


    Confusion matrix:
    [[8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
     [0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
     [0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
     [0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
     [0 0 0 0 6 0 0 0 0 0 0 0 0 0 0 0 0 2 0 0]
     [0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0 0]
     [0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0 0]
     [0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0 0]
     [0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0 0 0]
     [0 0 0 0 0 0 0 2 0 6 0 0 0 0 0 0 0 0 0 0]
     [0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0 0]
     [0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0 0]
     [0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0 0]
     [0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0 0]
     [0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0 0 0]
     [1 0 0 0 0 0 0 0 0 0 0 0 3 0 0 4 0 0 0 0]
     [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0 0]
     [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8 0 0]
     [0 0 0 0 0 0 0 2 0 0 0 0 0 0 0 0 0 0 6 0]
     [0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 8]]
                  precision    recall  f1-score   support
    
               0       0.89      1.00      0.94         8
               1       1.00      1.00      1.00         8
               2       1.00      1.00      1.00         8
               3       1.00      1.00      1.00         8
               4       1.00      0.75      0.86         8
               5       1.00      1.00      1.00         8
               6       1.00      1.00      1.00         8
               7       0.67      1.00      0.80         8
               8       1.00      1.00      1.00         8
               9       1.00      0.75      0.86         8
              10       1.00      1.00      1.00         8
              11       1.00      1.00      1.00         8
              12       0.73      1.00      0.84         8
              13       1.00      1.00      1.00         8
              14       1.00      1.00      1.00         8
              15       1.00      0.50      0.67         8
              16       1.00      1.00      1.00         8
              17       0.80      1.00      0.89         8
              18       1.00      0.75      0.86         8
              19       1.00      1.00      1.00         8
    
        accuracy                           0.94       160
       macro avg       0.95      0.94      0.94       160
    weighted avg       0.95      0.94      0.94       160
    
    

